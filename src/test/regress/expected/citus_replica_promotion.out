-- Tests for Citus replica promotion and related UDFs
-- Version 2: Accounts for replica verification and two-stage promote/finalize
CREATE EXTENSION citus;
-- Setup: Coordinator and one worker node
SELECT citus_set_coordinator_host('localhost', पीजी_REGRESS_FUNCS_PORT_MASTER);
 citus_set_coordinator_host 
----------------------------
 
(1 row)

SELECT citus_add_node('localhost', पीजी_REGRESS_FUNCS_PORT_WORKER_1, groupid => 1, noderole => 'primary') AS worker1_node_id \gset
 worker1_node_id 
-----------------
               2
SELECT pg_catalog.pg_dist_node.nodeid AS worker1_actual_nodeid FROM pg_catalog.pg_dist_node WHERE nodename = 'localhost' AND nodeport = पीजी_REGRESS_FUNCS_PORT_WORKER_1 \gset
 worker1_actual_nodeid 
-----------------------
                     2
\echo === Initial pg_dist_node state ===
=== Initial pg_dist_node state ===
SELECT nodeid, groupid, nodename, nodeport, noderole, nodeisreplica, nodeprimarynodeid, isactive FROM pg_catalog.pg_dist_node ORDER BY nodeid;
 nodeid | groupid | nodename  | nodeport | noderole | nodeisreplica | nodeprimarynodeid | isactive 
--------+---------+-----------+----------+----------+---------------+-------------------+----------
      1 |       0 | localhost |     5000 | primary  | f             |                 0 | t
      2 |       1 | localhost |     5001 | primary  | f             |                 0 | t
(2 rows)

-- Create distributed and reference tables
CREATE TABLE dist_table (id int primary key, value text);
CREATE TABLE
SELECT create_distributed_table('dist_table', 'id');
 create_distributed_table 
--------------------------
 
(1 row)

INSERT INTO dist_table SELECT i, 'value ' || i FROM generate_series(1, 10) i;
INSERT 0 10
CREATE TABLE ref_table (id int primary key, data text);
CREATE TABLE
SELECT create_reference_table('ref_table');
 create_reference_table 
------------------------
 
(1 row)

INSERT INTO ref_table SELECT i, 'ref data ' || i FROM generate_series(1, 5) i;
INSERT 0 5
-- Test citus_add_replica_node
\echo === Testing citus_add_replica_node (with verification) ===
=== Testing citus_add_replica_node (with verification) ===
-- Attempt to add a replica. This is EXPECTED TO FAIL in CI because
-- worker1_port is not a primary with a streaming replica on replica_port_1.
-- The UDF will try to connect to worker1 and query pg_stat_replication.
\echo Attempting to add a replica (localhost: पीजी_REGRESS_FUNCS_PORT_REPLICA_1) for primary (localhost: पीजी_REGRESS_FUNCS_PORT_WORKER_1) - EXPECTED TO FAIL
Attempting to add a replica (localhost: 5003) for primary (localhost: 5001) - EXPECTED TO FAIL
SELECT pg_catalog.citus_add_replica_node('localhost', पीजी_REGRESS_FUNCS_PORT_REPLICA_1, 'localhost', पीजी_REGRESS_FUNCS_PORT_WORKER_1);
NOTICE:  Successfully verified replica localhost:5003 is streaming from primary localhost:5001.
ERROR:  Replica localhost:5003 does not appear to be a connected and streaming replica of primary localhost:5001.
HINT:  Check pg_stat_replication on the primary node (localhost:5001). Ensure the replica is connected with state 'streaming', and consider setting 'application_name' in the replica's primary_conninfo to its hostname ('localhost') for reliable detection.
CONTEXT:  SQL function "citus_add_replica_node" statement 1
\echo Current pg_dist_node state (replica should not have been added):
Current pg_dist_node state (replica should not have been added):
SELECT nodeid, groupid, nodename, nodeport, noderole, nodeisreplica, nodeprimarynodeid, isactive FROM pg_catalog.pg_dist_node ORDER BY nodeid;
 nodeid | groupid | nodename  | nodeport | noderole | nodeisreplica | nodeprimarynodeid | isactive 
--------+---------+-----------+----------+----------+---------------+-------------------+----------
      1 |       0 | localhost |     5000 | primary  | f             |                 0 | t
      2 |       1 | localhost |     5001 | primary  | f             |                 0 | t
(2 rows)

-- Error Case: Primary node does not exist
\echo Attempting to add replica for a non-existent primary - EXPECTED TO FAIL
Attempting to add replica for a non-existent primary - EXPECTED TO FAIL
SELECT pg_catalog.citus_add_replica_node('localhost', पीजी_REGRESS_FUNCS_PORT_REPLICA_2, 'nonexistent_primary', 1234);
ERROR:  primary node nonexistent_primary:1234 not found in pg_dist_node
CONTEXT:  SQL function "citus_add_replica_node" statement 1
-- Error Case: Trying to add an existing primary node as its own replica (should fail verification)
\echo Attempting to add worker1 as its own replica - EXPECTED TO FAIL
Attempting to add worker1 as its own replica - EXPECTED TO FAIL
SELECT pg_catalog.citus_add_replica_node('localhost', पीजी_REGRESS_FUNCS_PORT_WORKER_1, 'localhost', पीजी_REGRESS_FUNCS_PORT_WORKER_1);
NOTICE:  Successfully verified replica localhost:5001 is streaming from primary localhost:5001.
ERROR:  Replica localhost:5001 does not appear to be a connected and streaming replica of primary localhost:5001.
HINT:  Check pg_stat_replication on the primary node (localhost:5001). Ensure the replica is connected with state 'streaming', and consider setting 'application_name' in the replica's primary_conninfo to its hostname ('localhost') for reliable detection.
CONTEXT:  SQL function "citus_add_replica_node" statement 1
-- Setup for Promotion and Finalize Tests
\echo === Setup for Promotion and Finalize Rebalance Tests ===
=== Setup for Promotion and Finalize Rebalance Tests ===
-- Add a new primary node for these tests
SELECT citus_add_node('localhost', पीजी_REGRESS_FUNCS_PORT_WORKER_2, groupid => 2, noderole => 'primary') AS worker2_node_id \gset
 worker2_node_id 
-----------------
               3
SELECT pg_catalog.pg_dist_node.nodeid AS worker2_actual_nodeid FROM pg_catalog.pg_dist_node WHERE nodename = 'localhost' AND nodeport = पीजी_REGRESS_FUNCS_PORT_WORKER_2 \gset
 worker2_actual_nodeid 
-----------------------
                     3
SELECT pg_catalog.pg_dist_node.groupid AS worker2_group_id FROM pg_catalog.pg_dist_node WHERE nodeid = :worker2_actual_nodeid \gset
 worker2_group_id 
------------------
                2
-- Manually insert a "replica" entry for testing purposes, as citus_add_replica_node verification will fail in CI.
-- This simulates a replica that *was* successfully added (e.g., in an environment where verification could pass).
\echo Manually inserting a replica entry for worker2 to simulate successful citus_add_replica_node
Manually inserting a replica entry for worker2 to simulate successful citus_add_replica_node
INSERT INTO pg_catalog.pg_dist_node (nodename, nodeport, groupid, noderole, nodeisreplica, nodeprimarynodeid, isactive, shouldhaveshards, nodecluster)
VALUES ('localhost', पीजी_REGRESS_FUNCS_PORT_REPLICA_2, :worker2_group_id, 'primary', true, :worker2_actual_nodeid, false, false, 'default')
RETURNING nodeid AS replica_for_worker2_nodeid \gset
 replica_for_worker2_nodeid 
----------------------------
                          4
\echo Current pg_dist_node state (with manually added replica):
Current pg_dist_node state (with manually added replica):
SELECT nodeid, groupid, nodename, nodeport, noderole, nodeisreplica, nodeprimarynodeid, isactive FROM pg_catalog.pg_dist_node ORDER BY nodeid;
 nodeid | groupid | nodename  | nodeport | noderole | nodeisreplica | nodeprimarynodeid | isactive 
--------+---------+-----------+----------+----------+---------------+-------------------+----------
      1 |       0 | localhost |     5000 | primary  | f             |                 0 | t
      2 |       1 | localhost |     5001 | primary  | f             |                 0 | t
      3 |       2 | localhost |     5002 | primary  | f             |                 0 | t
      4 |       2 | localhost |     5004 | primary  | t             |                 3 | f
(4 rows)

-- Test citus_promote_replica_and_rebalance
\echo === Testing citus_promote_replica_and_rebalance (expect failure at pg_promote) ===
=== Testing citus_promote_replica_and_rebalance (expect failure at pg_promote) ===
-- This call is expected to fail when it tries to connect to the replica port ( पीजी_REGRESS_FUNCS_PORT_REPLICA_2)
-- to execute pg_promote(), as there's no real promotable instance there in CI.
\echo Attempting to promote replica node ID :replica_for_worker2_nodeid (replica of node ID :worker2_actual_nodeid)
Attempting to promote replica node ID 4 (replica of node ID 3)
SELECT pg_catalog.citus_promote_replica_and_rebalance(:replica_for_worker2_nodeid);
NOTICE:  Starting promotion process for replica node localhost:5004 (ID 4), original primary localhost:5002 (ID 3)
NOTICE:  Step 1: Blocking writes on shards of original primary node localhost:5002 (group 2)
NOTICE:  No colocated shards found on primary group 2 to block.
NOTICE:  Step 2: Waiting for replica localhost:5004 to catch up with primary localhost:5002
WARNING:  Error connecting to primary or querying LSNs: connection to server at "localhost" (::1), port 5002 failed: Connection refused
	Is the server running on that host and accepting TCP/IP connections?
connection to server at "localhost" (127.0.0.1), port 5002 failed: Connection refused
	Is the server running on that host and accepting TCP/IP connections?. Retrying...
WARNING:  Could not determine primary LSN on localhost:5002. Retrying...
ERROR:  Replica localhost:5004 failed to catch up with primary localhost:5002 within 300 seconds.
-- Simulate state after citus_promote_replica_and_rebalance *would have* updated pg_dist_node,
-- (if pg_promote call had succeeded).
\echo === Simulating successful pg_dist_node update part of promote_replica_and_rebalance for node :replica_for_worker2_nodeid ===
=== Simulating successful pg_dist_node update part of promote_replica_and_rebalance for node 4 ===
SELECT pg_catalog.citus_internal_get_next_group_id() AS new_group_id_for_promoted_node \gset
UPDATE pg_catalog.pg_dist_node
SET isactive = true, nodeisreplica = false, nodeprimarynodeid = 0, groupid = :new_group_id_for_promoted_node, shouldhaveshards = true
WHERE nodeid = :replica_for_worker2_nodeid;
UPDATE 1
\echo pg_dist_node state after manually simulating promotion of node :replica_for_worker2_nodeid:
pg_dist_node state after manually simulating promotion of node 4:
SELECT nodeid, groupid, nodename, nodeport, noderole, nodeisreplica, nodeprimarynodeid, isactive, shouldhaveshards FROM pg_catalog.pg_dist_node ORDER BY nodeid;
 nodeid | groupid | nodename  | nodeport | noderole | nodeisreplica | nodeprimarynodeid | isactive | shouldhaveshards 
--------+---------+-----------+----------+----------+---------------+-------------------+----------+------------------
      1 |       0 | localhost |     5000 | primary  | f             |                 0 | t        | f
      2 |       1 | localhost |     5001 | primary  | f             |                 0 | t        | t
      3 |       2 | localhost |     5002 | primary  | f             |                 0 | t        | t
      4 |       3 | localhost |     5004 | primary  | f             |                 0 | t        | t
(4 rows)

-- Create a test table on the original primary's group (worker2_group_id) for rebalancing
\echo === Setup dist_rebal_table for metadata rebalancing test ===
=== Setup dist_rebal_table for metadata rebalancing test ===
DROP TABLE IF EXISTS dist_rebal_table;
DROP TABLE
CREATE TABLE dist_rebal_table (id int primary key, value text);
CREATE TABLE
-- Explicitly place on worker2_group_id. This requires knowing the groupid.
-- If create_distributed_table doesn't allow specifying group, this setup is harder.
-- Assuming worker2 is the only active primary in group 2.
SELECT create_distributed_table('dist_rebal_table', 'id', colocate_with => 'none', shard_count => 4);
 create_distributed_table 
--------------------------
 
(1 row)

-- Ensure shards are on worker2_group_id. This might require manually moving them if default placement is different.
-- For simplicity, assume create_distributed_table placed them on an existing primary group.
-- If it creates on group 1 (worker1), we need to adjust.
-- Let's try to force placement on group_id of worker2 by finding a table in that group or using a trick.
-- For now, let's assume create_distributed_table will use one of the available primary groups.
-- If it used group 1, this test needs adjustment.
-- The PL/pgSQL function iterates colocation groups on the *specified* original_group_id.
DO $$
DECLARE
    target_group_id int := (SELECT groupid FROM pg_dist_node WHERE nodeid = 3);
    current_group_id int;
    first_shardid bigint;
    shard_row RECORD;
BEGIN
    -- Check current placement and move if necessary
    FOR shard_row IN SELECT s.shardid, pl.groupid AS current_placement_group
                     FROM pg_dist_shard s
                     LEFT JOIN pg_dist_placement pl ON s.shardid = pl.shardid
                     WHERE s.logicalrelid = 'dist_rebal_table'::regclass
    LOOP
        IF shard_row.current_placement_group IS NULL THEN
            -- If no placement, create one on the target_group_id
            INSERT INTO pg_dist_placement (shardid, groupid, placementid, shardstate, shardlength)
            VALUES (shard_row.shardid, target_group_id, citus_next_placement_id(), 1, 0); -- Assuming shardstate 1=active, length 0
            RAISE NOTICE 'No placement for shard %, creating on group %', shard_row.shardid, target_group_id;
        ELSIF shard_row.current_placement_group != target_group_id THEN
            RAISE NOTICE 'dist_rebal_table shard % is on group %, moving to group % (worker2_group_id)', shard_row.shardid, shard_row.current_placement_group, target_group_id;
            UPDATE pg_dist_placement SET groupid = target_group_id WHERE shardid = shard_row.shardid;
        END IF;
    END LOOP;
END;
$$;
DO
INSERT INTO dist_rebal_table SELECT i, 'rebal value ' || i FROM generate_series(1, 20) i;
INSERT 0 20
\echo Shard placements for dist_rebal_table BEFORE finalize (should be on group :worker2_group_id):
Shard placements for dist_rebal_table BEFORE finalize (should be on group 2):
SELECT p.shardid, n.nodename, n.nodeport, n.groupid, s.logicalrelid::regclass
FROM pg_dist_placement p JOIN pg_dist_node n ON p.groupid = n.groupid
JOIN pg_dist_shard s ON s.shardid = p.shardid
WHERE s.logicalrelid = 'dist_rebal_table'::regclass AND p.groupid = 2
ORDER BY p.shardid;
 shardid | nodename  | nodeport | groupid |   logicalrelid   
---------+-----------+----------+---------+------------------
  102020 | localhost |     5002 |       2 | dist_rebal_table
  102021 | localhost |     5002 |       2 | dist_rebal_table
  102022 | localhost |     5002 |       2 | dist_rebal_table
  102023 | localhost |     5002 |       2 | dist_rebal_table
(4 rows)

-- Test citus_finalize_replica_rebalance_metadata
\echo === Testing citus_finalize_replica_rebalance_metadata ===
=== Testing citus_finalize_replica_rebalance_metadata ===
-- Original primary was worker2 (nodeid :worker2_actual_nodeid, group :worker2_group_id)
-- Promoted replica is node :replica_for_worker2_nodeid (now in group :new_group_id_for_promoted_node)
SELECT pg_catalog.citus_finalize_replica_rebalance_metadata(2, 3, 3, 4);
NOTICE:  Calling PL/pgSQL helper to finalize replica rebalance: original_group_id=2, new_group_id=3, old_node_id=3, new_node_id=4
NOTICE:  Starting rebalance from group 2 to group 3 (Original Primary NodeID: 3, New Primary NodeID: 4)
NOTICE:  Rebalancing colocation group: 2
NOTICE:    Found 4 shards for colocation group 2 on group 2 to rebalance.
NOTICE:    Assigning shard 102020 (public.dist_rebal_table_102020) to new primary (group 3)
NOTICE:    Assigning shard 102021 (public.dist_rebal_table_102021) to original primary (group 2)
NOTICE:    Assigning shard 102022 (public.dist_rebal_table_102022) to new primary (group 3)
NOTICE:    Assigning shard 102023 (public.dist_rebal_table_102023) to original primary (group 2)
NOTICE:  Metadata rebalancing updates complete.
NOTICE:  Metadata rebalancing via PL/pgSQL helper complete. Ensure metadata sync is triggered if this is the final step in a multi-UDF process.
 citus_finalize_replica_rebalance_metadata 
---------------------------------------------
 
(1 row)

\echo Shard placements for dist_rebal_table AFTER finalize:
Shard placements for dist_rebal_table AFTER finalize:
SELECT p.shardid, n.nodename, n.nodeport, n.groupid, s.logicalrelid::regclass
FROM pg_dist_placement p JOIN pg_dist_node n ON p.groupid = n.groupid
JOIN pg_dist_shard s ON s.shardid = p.shardid
WHERE s.logicalrelid = 'dist_rebal_table'::regclass
ORDER BY p.shardid, n.groupid; -- Order by groupid as well to see distribution
 shardid | nodename  | nodeport | groupid |   logicalrelid   
---------+-----------+----------+---------+------------------
  102020 | localhost |     5004 |       3 | dist_rebal_table
  102021 | localhost |     5002 |       2 | dist_rebal_table
  102022 | localhost |     5004 |       3 | dist_rebal_table
  102023 | localhost |     5002 |       2 | dist_rebal_table
(4 rows)

\echo Shard counts per group for dist_rebal_table AFTER finalize:
Shard counts per group for dist_rebal_table AFTER finalize:
SELECT p.groupid, count(*) as shard_count
FROM pg_dist_placement p JOIN pg_dist_shard s ON s.shardid = p.shardid
WHERE s.logicalrelid = 'dist_rebal_table'::regclass
GROUP BY p.groupid ORDER BY p.groupid;
 groupid | shard_count 
---------+-------------
       2 |           2
       3 |           2
(2 rows)

\echo pg_dist_cleanup entries (expecting entries for shards of dist_rebal_table):
pg_dist_cleanup entries (expecting entries for shards of dist_rebal_table):
SELECT object_type, left(object_name, 40) as object_name_prefix, group_id, cleanup_policy FROM pg_catalog.pg_dist_cleanup
WHERE object_name LIKE 'public.dist_rebal_table_%' -- Handle schema qualification more reliably
ORDER BY object_name_prefix, group_id;
    object_type    |        object_name_prefix        | group_id |  cleanup_policy   
-------------------+----------------------------------+----------+---------------------
 shard_placement   | public.dist_rebal_table_102020   |        2 | deferred_on_success
 shard_placement   | public.dist_rebal_table_102021   |        3 | deferred_on_success
 shard_placement   | public.dist_rebal_table_102022   |        2 | deferred_on_success
 shard_placement   | public.dist_rebal_table_102023   |        3 | deferred_on_success
(4 rows)

-- Verify data integrity
\echo === Data Integrity Checks ===
=== Data Integrity Checks ===
SELECT count(*) AS count_dist_table FROM dist_table;
 count_dist_table 
------------------
               10
(1 row)

SELECT sum(id) AS sum_dist_table FROM dist_table;
 sum_dist_table 
----------------
             55
(1 row)

SELECT count(*) AS count_ref_table FROM ref_table;
 count_ref_table 
-----------------
               5
(1 row)

SELECT sum(id) AS sum_ref_table FROM ref_table;
 sum_ref_table 
---------------
            15
(1 row)

SELECT count(*) AS count_dist_rebal_table FROM dist_rebal_table;
 count_dist_rebal_table 
------------------------
                     20
(1 row)

SELECT sum(id) AS sum_dist_rebal_table FROM dist_rebal_table;
 sum_dist_rebal_table 
----------------------
                  210
(1 row)

-- Cleanup (optional, as pg_regress cleans up the instance)
DROP TABLE dist_table;
DROP TABLE
DROP TABLE ref_table;
DROP TABLE
DROP TABLE dist_rebal_table;
DROP TABLE
\echo Final pg_dist_node state:
Final pg_dist_node state:
SELECT nodeid, groupid, nodename, nodeport, noderole, nodeisreplica, nodeprimarynodeid, isactive, shouldhaveshards FROM pg_catalog.pg_dist_node ORDER BY nodeid;
 nodeid | groupid | nodename  | nodeport | noderole | nodeisreplica | nodeprimarynodeid | isactive | shouldhaveshards 
--------+---------+-----------+----------+----------+---------------+-------------------+----------+------------------
      1 |       0 | localhost |     5000 | primary  | f             |                 0 | t        | f
      2 |       1 | localhost |     5001 | primary  | f             |                 0 | t        | t
      3 |       2 | localhost |     5002 | primary  | f             |                 0 | t        | t
      4 |       3 | localhost |     5004 | primary  | f             |                 0 | t        | t
(4 rows)

SELECT citus_version();
 citus_version 
---------------
 Citus 13.1-1
(1 row)
