-- Setup: Create distributed tables and add worker nodes
SELECT master_add_node('localhost', 5432);
 master_add_node 
-----------------
               1
(1 row)

SELECT master_add_node('localhost', 5433);
 master_add_node 
-----------------
               2
(1 row)

SELECT master_add_node('localhost', 5434); -- Third node for some tests
 master_add_node 
-----------------
               3
(1 row)

-- Create some distributed tables
CREATE TABLE dist_table1 (id int primary key, val text);
CREATE TABLE
SELECT create_distributed_table('dist_table1', 'id');
 create_distributed_table 
--------------------------
 
(1 row)

CREATE TABLE dist_table2 (key int primary key, data jsonb);
CREATE TABLE
SELECT create_distributed_table('dist_table2', 'key');
 create_distributed_table 
--------------------------
 
(1 row)

-- Insert some data to create shards
INSERT INTO dist_table1 SELECT i, 'value_' || i FROM generate_series(1, 20) i;
INSERT 0 20
INSERT INTO dist_table2 SELECT i, jsonb_build_object('i', i) FROM generate_series(1, 10) i;
INSERT 0 10

-- Initially, all shards will be on the first node (5432) or distributed by default placement strategy.
-- For consistent testing, let's move some shards explicitly to node1 (localhost:5432)
-- This requires knowing shard names, which can be complex.
-- A simpler approach for testing is to assume default strategy places them on the first available node
-- or use rebalance_table_shards to consolidate on one node first if needed.
-- For this test, we'll assume shards are somewhat distributed and then test moving specific ones.

-- Manually verify initial shard distribution if tests are flaky.
SELECT table_name, shardid, nodename, nodeport FROM pg_dist_placement WHERE nodename = 'localhost' AND nodeport = 5432 ORDER BY shardid;
 table_name | shardid | nodename  | nodeport 
------------+---------+-----------+----------
 dist_table1|  102040 | localhost |     5432
 dist_table1|  102041 | localhost |     5432
 dist_table1|  102042 | localhost |     5432
 dist_table1|  102043 | localhost |     5432
 dist_table2|  102044 | localhost |     5432
 dist_table2|  102045 | localhost |     5432
 dist_table2|  102046 | localhost |     5432
 dist_table2|  102047 | localhost |     5432
(8 rows)

-- Test 1: Basic rebalance from node1 to node2 (empty)
-- Node1: localhost:5432, Node2: localhost:5433
SELECT table_name, shardid, sourcename, sourceport, targetname, targetport, action
FROM pg_catalog.get_rebalance_plan_for_two_nodes('localhost', 5432, 'localhost', 5433)
ORDER BY shardid;
 table_name  | shardid | sourcename | sourceport | targetname | targetport | action 
-------------+---------+------------+------------+------------+------------+--------
 dist_table1 |  102040 | localhost  |       5432 | localhost  |       5433 | MOVE
 dist_table1 |  102041 | localhost  |       5432 | localhost  |       5433 | MOVE
 dist_table1 |  102042 | localhost  |       5432 | localhost  |       5432 | STAY
 dist_table1 |  102043 | localhost  |       5432 | localhost  |       5432 | STAY
 dist_table2 |  102044 | localhost  |       5432 | localhost  |       5433 | MOVE
 dist_table2 |  102045 | localhost  |       5432 | localhost  |       5433 | MOVE
 dist_table2 |  102046 | localhost  |       5432 | localhost  |       5432 | STAY
 dist_table2 |  102047 | localhost  |       5432 | localhost  |       5432 | STAY
(8 rows)

-- Manually move some shards to node 2 based on the plan for next tests
-- This is tricky as shardids are dynamic.
-- Instead, let's test with existing distribution and then specific table.

-- Test 2: Rebalance for a specific table (dist_table1)
SELECT table_name, shardid, sourcename, sourceport, targetname, targetport, action
FROM pg_catalog.get_rebalance_plan_for_two_nodes('localhost', 5432, 'localhost', 5433, 'dist_table1'::regclass)
ORDER BY shardid;
 table_name  | shardid | sourcename | sourceport | targetname | targetport | action 
-------------+---------+------------+------------+------------+------------+--------
 dist_table1 |  102040 | localhost  |       5432 | localhost  |       5433 | MOVE
 dist_table1 |  102041 | localhost  |       5432 | localhost  |       5433 | MOVE
 dist_table1 |  102042 | localhost  |       5432 | localhost  |       5432 | STAY
 dist_table1 |  102043 | localhost  |       5432 | localhost  |       5432 | STAY
(4 rows)

-- Test 3: Rebalance with excluded shards
-- To get some shard IDs to exclude:
CREATE TEMP TABLE all_shards_on_node1 AS
SELECT shardid FROM pg_dist_shard_placement
JOIN pg_dist_shard USING (shardid)
WHERE nodename = 'localhost' AND nodeport = 5432 AND logicalrelid = 'dist_table1'::regclass; -- only from dist_table1 for predictability
CREATE TABLE
CREATE TEMP TABLE excluded_ids AS SELECT shardid FROM all_shards_on_node1 LIMIT 1; -- Exclude 1 shard from dist_table1
CREATE TABLE
SELECT * FROM excluded_ids; -- show excluded ids
 shardid 
---------
  102040
(1 row)

SELECT table_name, shardid, sourcename, sourceport, targetname, targetport, action
FROM pg_catalog.get_rebalance_plan_for_two_nodes(
    'localhost', 5432, 'localhost', 5433, 'dist_table1'::regclass,
    excluded_shard_list := (SELECT array_agg(shardid) FROM excluded_ids)
)
ORDER BY shardid;
 table_name  | shardid | sourcename | sourceport | targetname | targetport | action 
-------------+---------+------------+------------+------------+------------+--------
 dist_table1 |  102041 | localhost  |       5432 | localhost  |       5433 | MOVE
 dist_table1 |  102042 | localhost  |       5432 | localhost  |       5432 | STAY
 dist_table1 |  102043 | localhost  |       5432 | localhost  |       5432 | STAY
(3 rows)

DROP TABLE excluded_ids;
DROP TABLE
DROP TABLE all_shards_on_node1;
DROP TABLE


-- Test 4: Source node has no shards for a specific table
CREATE TABLE dist_empty_source_test (id int primary key);
CREATE TABLE
SELECT create_distributed_table('dist_empty_source_test', 'id');
 create_distributed_table 
--------------------------
 
(1 row)

-- Ensure its shards are NOT on localhost:5432 by moving them if they are
-- For simplicity, we assume it's not there or the function handles it.
-- A better test would be to ensure all shards of this table are on node 5434.
DO $$
DECLARE
    v_shardid bigint;
    v_nodename text;
    v_nodeport int;
BEGIN
    FOR v_shardid, v_nodename, v_nodeport IN SELECT shardid, nodename, nodeport FROM pg_dist_placement WHERE logicalrelid = 'dist_empty_source_test'::regclass LOOP
        IF v_nodename = 'localhost' AND v_nodeport = 5432 THEN
            PERFORM master_move_shard_placement(v_shardid, 'localhost', 5432, 'localhost', 5434);
        END IF;
    END LOOP;
END;
$$;
DO

SELECT table_name, shardid, sourcename, sourceport, targetname, targetport, action
FROM pg_catalog.get_rebalance_plan_for_two_nodes('localhost', 5432, 'localhost', 5433, 'dist_empty_source_test'::regclass)
ORDER BY shardid;
 table_name | shardid | sourcename | sourceport | targetname | targetport | action 
------------+---------+------------+------------+------------+------------+--------
(0 rows)

-- Test 5: Target node is the same as source node (should produce no plan, all STAY)
SELECT table_name, shardid, sourcename, sourceport, targetname, targetport, action
FROM pg_catalog.get_rebalance_plan_for_two_nodes('localhost', 5432, 'localhost', 5432, 'dist_table1'::regclass) -- specify table for predictable output
ORDER BY shardid;
 table_name  | shardid | sourcename | sourceport | targetname | targetport | action 
-------------+---------+------------+------------+------------+------------+--------
 dist_table1 |  102040 | localhost  |       5432 | localhost  |       5432 | STAY
 dist_table1 |  102041 | localhost  |       5432 | localhost  |       5432 | STAY
 dist_table1 |  102042 | localhost  |       5432 | localhost  |       5432 | STAY
 dist_table1 |  102043 | localhost  |       5432 | localhost  |       5432 | STAY
(4 rows)

-- Test 6: Using a different rebalance strategy (disallowing moves to target)
INSERT INTO pg_dist_rebalance_strategy VALUES
('no_move_to_target_strategy', false, 'citus_shard_cost_by_disk_size', 'citus_node_capacity_by_disk_size', 'custom_shard_allowed_on_node', 0.1, 0.1);
INSERT 0 1

CREATE OR REPLACE FUNCTION custom_shard_allowed_on_node(p_shard_id bigint, p_node_id int)
RETURNS boolean LANGUAGE plpgsql AS $$
DECLARE
    v_target_node_id int;
BEGIN
    SELECT n.nodeid INTO v_target_node_id FROM pg_dist_node n WHERE n.nodename = 'localhost' AND n.nodeport = 5433 LIMIT 1;
    IF p_node_id = v_target_node_id THEN
        RETURN false; 
    END IF;
    RETURN true;
END;
$$;
CREATE FUNCTION

SELECT table_name, shardid, sourcename, sourceport, targetname, targetport, action
FROM pg_catalog.get_rebalance_plan_for_two_nodes('localhost', 5432, 'localhost', 5433, 'dist_table1'::regclass, rebalance_strategy => 'no_move_to_target_strategy')
ORDER BY shardid;
 table_name  | shardid | sourcename | sourceport | targetname | targetport | action 
-------------+---------+------------+------------+------------+------------+--------
 dist_table1 |  102040 | localhost  |       5432 | localhost  |       5432 | STAY
 dist_table1 |  102041 | localhost  |       5432 | localhost  |       5432 | STAY
 dist_table1 |  102042 | localhost  |       5432 | localhost  |       5432 | STAY
 dist_table1 |  102043 | localhost  |       5432 | localhost  |       5432 | STAY
(4 rows)

-- Clean up custom strategy and function
DELETE FROM pg_dist_rebalance_strategy WHERE strategy_name = 'no_move_to_target_strategy';
DELETE 1
DROP FUNCTION custom_shard_allowed_on_node(bigint, int);
DROP FUNCTION

-- Test 7: No distributed tables exist (after dropping them)
DROP TABLE dist_table1;
DROP TABLE
DROP TABLE dist_table2;
DROP TABLE
DROP TABLE dist_empty_source_test;
DROP TABLE

SELECT table_name, shardid, sourcename, sourceport, targetname, targetport, action
FROM pg_catalog.get_rebalance_plan_for_two_nodes('localhost', 5432, 'localhost', 5433)
ORDER BY shardid;
 table_name | shardid | sourcename | sourceport | targetname | targetport | action 
------------+---------+------------+------------+------------+------------+--------
(0 rows)

-- Clean up nodes
-- Do this last after all tests that might use these nodes.
SELECT master_remove_node('localhost', 5434);
 master_remove_node 
--------------------
               0
(1 row)

SELECT master_remove_node('localhost', 5433);
 master_remove_node 
--------------------
               0
(1 row)

SELECT master_remove_node('localhost', 5432);
 master_remove_node 
--------------------
               0
(1 row)
